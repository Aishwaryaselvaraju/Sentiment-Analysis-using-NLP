# Importing necessary libraries
import pandas as pd
import numpy as np
import nltk
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import matplotlib.pyplot as plt

# Download required NLTK resources
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# Step 1: Load Dataset (replace with your dataset path or URL)
df = pd.read_csv('sentiment_data.csv')  # Replace with the actual dataset

# Example of dataset structure:
# | text                               | sentiment |
# | "Great product, highly recommend!" | positive  |
# | "Worst purchase I ever made"       | negative  |
# | "It's okay, nothing special"       | neutral   |

# Display the first few rows of the dataset
print(df.head())

# Step 2: Data Preprocessing

# Text Cleaning: Tokenization, Stopword Removal, and Lemmatization
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def preprocess_text(text):
    # Tokenization
    tokens = word_tokenize(text.lower())
    
    # Remove stopwords and lemmatize
    cleaned_tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalpha() and word not in stop_words]
    
    return " ".join(cleaned_tokens)

# Apply preprocessing to the dataset
df['cleaned_text'] = df['text'].apply(preprocess_text)

# Step 3: Split Dataset into Training and Testing
X = df['cleaned_text']  # Features
y = df['sentiment']     # Target variable

# Train-test split (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 4: Feature Extraction - Using TF-IDF
tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # Limit to top 5000 features
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
X_test_tfidf = tfidf_vectorizer.transform(X_test)

# Step 5: Train Models - Logistic Regression and SVM
# Logistic Regression
lr_model = LogisticRegression(max_iter=1000)
lr_model.fit(X_train_tfidf, y_train)

# Support Vector Machine (SVM)
svm_model = SVC()
svm_model.fit(X_train_tfidf, y_train)

# Step 6: Evaluate the Models

# Logistic Regression Evaluation
y_pred_lr = lr_model.predict(X_test_tfidf)
print("\nLogistic Regression - Classification Report:\n", classification_report(y_test, y_pred_lr))
print("Logistic Regression - Accuracy: ", accuracy_score(y_test, y_pred_lr))

# SVM Evaluation
y_pred_svm = svm_model.predict(X_test_tfidf)
print("\nSVM - Classification Report:\n", classification_report(y_test, y_pred_svm))
print("SVM - Accuracy: ", accuracy_score(y_test, y_pred_svm))

# Confusion Matrix for Logistic Regression
cm_lr = confusion_matrix(y_test, y_pred_lr)
plt.figure(figsize=(5, 5))
plt.matshow(cm_lr, cmap='Blues', fignum=1)
plt.title('Confusion Matrix - Logistic Regression')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.colorbar()
plt.show()
